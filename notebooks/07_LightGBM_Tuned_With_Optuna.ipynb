{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a192b6",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "This is the May 2025 calories prediction competition.\n",
    "\n",
    "### Files\n",
    "1. train.csv\n",
    "2. test.csv\n",
    "3. sample_submission.csv\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "The evaluation metric is the RMSLE.\n",
    "\n",
    "Submission File\n",
    "For each id in the test set, you must predict the number of minutes listened. The file should contain a header and have the following format:\n",
    "\n",
    "- id,Listening_Time_minutes\n",
    "- 26570,0.2\n",
    "- 26571,0.1\n",
    "- 26572,0.9\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccae698",
   "metadata": {},
   "source": [
    "## Package Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7314a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general python libraries\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# dataframe and data manipulation library\n",
    "import pandas as pd\n",
    "\n",
    "# visualisation and EDA libraries\n",
    "import matplotlib.pyplot as  plt\n",
    "import seaborn as sns\n",
    "\n",
    "# machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import TargetEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669de298",
   "metadata": {},
   "source": [
    "## Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e5e28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN = 'Calories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7baff0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../data/raw'\n",
    "df_train = pd.read_csv(f'{folder_path}/train.csv')\n",
    "df_test = pd.read_csv(f'{folder_path}/test.csv')\n",
    "df_sample_submission = pd.read_csv(f'{folder_path}/sample_submission.csv',index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2f11ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Heart_Rate</th>\n",
       "      <th>Body_Temp</th>\n",
       "      <th>Calories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>36</td>\n",
       "      <td>189.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>64</td>\n",
       "      <td>163.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>39.7</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>51</td>\n",
       "      <td>161.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>20</td>\n",
       "      <td>192.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>40.7</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>166.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749995</th>\n",
       "      <td>749995</td>\n",
       "      <td>male</td>\n",
       "      <td>28</td>\n",
       "      <td>193.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>40.9</td>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749996</th>\n",
       "      <td>749996</td>\n",
       "      <td>female</td>\n",
       "      <td>64</td>\n",
       "      <td>165.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>40.5</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749997</th>\n",
       "      <td>749997</td>\n",
       "      <td>male</td>\n",
       "      <td>60</td>\n",
       "      <td>162.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>40.9</td>\n",
       "      <td>221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749998</th>\n",
       "      <td>749998</td>\n",
       "      <td>male</td>\n",
       "      <td>45</td>\n",
       "      <td>182.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>40.3</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749999</th>\n",
       "      <td>749999</td>\n",
       "      <td>female</td>\n",
       "      <td>39</td>\n",
       "      <td>171.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id     Sex  Age  Height  Weight  Duration  Heart_Rate  Body_Temp  \\\n",
       "0            0    male   36   189.0    82.0      26.0       101.0       41.0   \n",
       "1            1  female   64   163.0    60.0       8.0        85.0       39.7   \n",
       "2            2  female   51   161.0    64.0       7.0        84.0       39.8   \n",
       "3            3    male   20   192.0    90.0      25.0       105.0       40.7   \n",
       "4            4  female   38   166.0    61.0      25.0       102.0       40.6   \n",
       "...        ...     ...  ...     ...     ...       ...         ...        ...   \n",
       "749995  749995    male   28   193.0    97.0      30.0       114.0       40.9   \n",
       "749996  749996  female   64   165.0    63.0      18.0        92.0       40.5   \n",
       "749997  749997    male   60   162.0    67.0      29.0       113.0       40.9   \n",
       "749998  749998    male   45   182.0    91.0      17.0       102.0       40.3   \n",
       "749999  749999  female   39   171.0    65.0      19.0        97.0       40.6   \n",
       "\n",
       "        Calories  \n",
       "0          150.0  \n",
       "1           34.0  \n",
       "2           29.0  \n",
       "3          140.0  \n",
       "4          146.0  \n",
       "...          ...  \n",
       "749995     230.0  \n",
       "749996      96.0  \n",
       "749997     221.0  \n",
       "749998     109.0  \n",
       "749999     103.0  \n",
       "\n",
       "[750000 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b85c0179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Heart_Rate</th>\n",
       "      <th>Body_Temp</th>\n",
       "      <th>Calories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>36</td>\n",
       "      <td>189.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>64</td>\n",
       "      <td>163.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>39.7</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>51</td>\n",
       "      <td>161.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>20</td>\n",
       "      <td>192.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>40.7</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>166.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749995</th>\n",
       "      <td>749995</td>\n",
       "      <td>male</td>\n",
       "      <td>28</td>\n",
       "      <td>193.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>40.9</td>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749996</th>\n",
       "      <td>749996</td>\n",
       "      <td>female</td>\n",
       "      <td>64</td>\n",
       "      <td>165.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>40.5</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749997</th>\n",
       "      <td>749997</td>\n",
       "      <td>male</td>\n",
       "      <td>60</td>\n",
       "      <td>162.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>40.9</td>\n",
       "      <td>221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749998</th>\n",
       "      <td>749998</td>\n",
       "      <td>male</td>\n",
       "      <td>45</td>\n",
       "      <td>182.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>40.3</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749999</th>\n",
       "      <td>749999</td>\n",
       "      <td>female</td>\n",
       "      <td>39</td>\n",
       "      <td>171.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id     Sex  Age  Height  Weight  Duration  Heart_Rate  Body_Temp  \\\n",
       "0            0    male   36   189.0    82.0      26.0       101.0       41.0   \n",
       "1            1  female   64   163.0    60.0       8.0        85.0       39.7   \n",
       "2            2  female   51   161.0    64.0       7.0        84.0       39.8   \n",
       "3            3    male   20   192.0    90.0      25.0       105.0       40.7   \n",
       "4            4  female   38   166.0    61.0      25.0       102.0       40.6   \n",
       "...        ...     ...  ...     ...     ...       ...         ...        ...   \n",
       "749995  749995    male   28   193.0    97.0      30.0       114.0       40.9   \n",
       "749996  749996  female   64   165.0    63.0      18.0        92.0       40.5   \n",
       "749997  749997    male   60   162.0    67.0      29.0       113.0       40.9   \n",
       "749998  749998    male   45   182.0    91.0      17.0       102.0       40.3   \n",
       "749999  749999  female   39   171.0    65.0      19.0        97.0       40.6   \n",
       "\n",
       "        Calories  \n",
       "0          150.0  \n",
       "1           34.0  \n",
       "2           29.0  \n",
       "3          140.0  \n",
       "4          146.0  \n",
       "...          ...  \n",
       "749995     230.0  \n",
       "749996      96.0  \n",
       "749997     221.0  \n",
       "749998     109.0  \n",
       "749999     103.0  \n",
       "\n",
       "[750000 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84bf769d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              int64\n",
       "Sex            object\n",
       "Age             int64\n",
       "Height        float64\n",
       "Weight        float64\n",
       "Duration      float64\n",
       "Heart_Rate    float64\n",
       "Body_Temp     float64\n",
       "Calories      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe538dc",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc21fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \n",
    "    # Encode sex as binary flag\n",
    "    gender_mapping = {\n",
    "        'male': 0,\n",
    "        'female': 1\n",
    "    }\n",
    "    df['is_female'] = df['Sex'].map(gender_mapping)\n",
    "    df = df.drop(columns=['Sex'])\n",
    "\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f439d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X: pd.DataFrame):\n",
    "\n",
    "    X = feature_engineering(X)\n",
    "\n",
    "    return X # Enabled this to stop warnings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "308af2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoding(X_train, y_train, X_test):\n",
    "\n",
    "    ### TARGET ENCODING\n",
    "    # Categorical Columns\n",
    "    categorical_columns = [\"Genre\",\"Publication_Day\",\"Episode_Sentiment\",\"Publication_Time\",\"Podcast_Name\"]\n",
    "    categorical_encoded_columns = [column_name + '_TE' for column_name in categorical_columns]\n",
    "\n",
    "    encoder = TargetEncoder(categories='auto', smooth='auto', cv=5, random_state=42)\n",
    "    encoder.fit(X_train[categorical_columns], y_train)\n",
    "    X_train[categorical_encoded_columns] = encoder.transform(X_train[categorical_columns])\n",
    "    X_test[categorical_encoded_columns] = encoder.transform(X_test[categorical_columns])    \n",
    "\n",
    "    # # Interaction Columns\n",
    "    # interaction_features = [\n",
    "    #     ('Publication_Day','Publication_Time')\n",
    "    # ]\n",
    "\n",
    "    # interaction_features_to_be_encoded = []\n",
    "    # for feature_1, feature_2 in interaction_features:\n",
    "    #     feature_name = feature_1 + '_' + feature_2 + '_TE'\n",
    "    #     X_train[feature_name] = (X_train[feature_1].astype('str') + '_' + X_train[feature_2].astype('str')).astype('category')\n",
    "    #     X_test[feature_name] = (X_test[feature_1].astype('str') + '_' + X_test[feature_2].astype('str')).astype('category')\n",
    "    #     interaction_features_to_be_encoded.append(feature_name)\n",
    "    \n",
    "    # encoder = TargetEncoder(categories='auto', smooth='auto', cv=5, random_state=42)\n",
    "    # encoder.fit(X_train[interaction_features_to_be_encoded], y_train)\n",
    "    # X_train[interaction_features_to_be_encoded] = encoder.transform(X_train[interaction_features_to_be_encoded])\n",
    "    # X_test[interaction_features_to_be_encoded] = encoder.transform(X_test[interaction_features_to_be_encoded])    \n",
    "\n",
    "    # # Fitting encoder and transforming data\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ff51658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(X:pd.DataFrame):\n",
    "\n",
    "    columns_to_drop = [\n",
    "        # 'Sex', # Already dropped in feature engineering\n",
    "        # 'Age',\n",
    "        # 'Height',\n",
    "        # 'Weight',\n",
    "        # 'Duration',\n",
    "        # 'Heart_Rate',\n",
    "        # 'Body_Temp',\n",
    "        # 'Calories' # target variable\n",
    "    ]\n",
    "\n",
    "    X = X.drop(columns=columns_to_drop)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f759e220",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "\n",
    "### Train Test Split\n",
    "\n",
    "Splitting data into groupings for model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84028e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_log_error\n",
    "\n",
    "def get_rmsle(preds,eval_data):\n",
    "\n",
    "    y_preds_negatives_removed = np.clip(preds, a_min= 0, a_max = np.inf)\n",
    "\n",
    "    rmsle = root_mean_squared_log_error(y_preds_negatives_removed, eval_data)\n",
    "\n",
    "    # Metric must have the following format to be accepted\n",
    "    # eval_name, eval_result, is_higher_better\n",
    "    return 'rmsle', rmsle, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab862e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: 0x30f0da180>\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: 0x30f0daa50>\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.062371\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000003 seconds, init for row-wise cost 0.003329 seconds\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 562500, number of used features: 8\n",
      "[LightGBM] [Debug] Use subset for bagging\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: 0x30f0dafc0>\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Info] Start training from score 88.270427\n",
      "[LightGBM] [Debug] Re-bagging, using 326569 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326123 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 327015 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325856 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325439 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326256 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325796 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326358 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325790 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326224 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326634 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325233 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326206 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326067 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326615 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326278 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326194 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326827 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326093 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326784 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] Unknown parameter: 0x15fd9e9f0>\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] Unknown parameter: 0x30f0dafc0>\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "--- Fold 1 Completed ---\n",
      "train_rmse, test_rmse -  0.06879785788186916 0.06925285720040308\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: 0x30f0d9e20>\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Warning] Unknown parameter: 0x30f0d8ef0>\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.062326\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000001 seconds, init for row-wise cost 0.002959 seconds\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 614\n",
      "[LightGBM] [Info] Number of data points in the train set: 562500, number of used features: 8\n",
      "[LightGBM] [Debug] Use subset for bagging\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: 0x30f0d9460>\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Info] Start training from score 88.239244\n",
      "[LightGBM] [Debug] Re-bagging, using 326569 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326123 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 327015 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325856 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325439 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326256 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325796 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326358 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325790 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326224 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326634 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325233 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326206 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326067 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326615 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 28 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326278 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326194 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326827 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326093 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326784 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] Unknown parameter: 0x30f0db650>\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] Unknown parameter: 0x30f0db380>\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "--- Fold 2 Completed ---\n",
      "train_rmse, test_rmse -  0.07017334501332882 0.06983062679245736\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: 0x311e2ba70>\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: 0x311e2b3e0>\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.062433\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000001 seconds, init for row-wise cost 0.002790 seconds\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 562500, number of used features: 8\n",
      "[LightGBM] [Debug] Use subset for bagging\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: 0x311e2a240>\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Info] Start training from score 88.296244\n",
      "[LightGBM] [Debug] Re-bagging, using 326569 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326123 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 327015 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325856 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325439 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326256 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325796 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326358 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325790 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326224 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326634 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325233 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326206 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326067 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326615 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326278 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326194 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326827 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326093 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326784 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] Unknown parameter: 0x30f0d0740>\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] Unknown parameter: 0x30f0d12e0>\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "--- Fold 3 Completed ---\n",
      "train_rmse, test_rmse -  0.07044029096517955 0.07112757220315687\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: 0x30f0d8e00>\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: 0x30f0db7d0>\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.062266\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000001 seconds, init for row-wise cost 0.002888 seconds\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 614\n",
      "[LightGBM] [Info] Number of data points in the train set: 562500, number of used features: 8\n",
      "[LightGBM] [Debug] Use subset for bagging\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: 0x30f0db380>\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Info] Start training from score 88.325211\n",
      "[LightGBM] [Debug] Re-bagging, using 326569 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326123 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 327015 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325856 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325439 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326256 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325796 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326358 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325790 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326224 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326634 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 325233 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326206 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326067 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326615 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326278 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326194 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326827 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326093 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 27 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 326784 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 27 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: 0x30f0db950>\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] Unknown parameter: object\n",
      "[LightGBM] [Warning] Unknown parameter: 0x30f0d8170>\n",
      "[LightGBM] [Warning] Unknown parameter: at\n",
      "[LightGBM] [Warning] Unknown parameter: callbacks\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "--- Fold 4 Completed ---\n",
      "train_rmse, test_rmse -  0.06886010721120295 0.06944034052908019\n",
      "--- Training_Completed ---\n",
      "The average test cross neg_root_mean_squared_error is  0.06991284918127437\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_validate, cross_val_score\n",
    "from sklearn.metrics import root_mean_squared_log_error\n",
    "from xgboost.callback import EarlyStopping\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "\n",
    "NUMBER_OF_SPLITS = 4\n",
    "    \n",
    "outer_kfold = KFold(n_splits=NUMBER_OF_SPLITS, shuffle=True)\n",
    "encoder = TargetEncoder(categories='auto', smooth='auto', cv=5, random_state=42)\n",
    "\n",
    "list_train_rmse = []\n",
    "list_test_rmse = []\n",
    "\n",
    "for fold_number, (infold_training_indices, infold_test_indices) in enumerate(outer_kfold.split(df_train), 1):\n",
    "\n",
    "    # Pre-processing of training data in kfold\n",
    "    X_train = df_train.loc[infold_training_indices,df_train.columns != TARGET_COLUMN]\n",
    "    y_train = df_train.loc[infold_training_indices,TARGET_COLUMN]\n",
    "\n",
    "    X_train = preprocessing(X_train)\n",
    "    X_train = postprocessing(X_train)\n",
    "\n",
    "    # Pre-processing of training data in kfold for in-fold validation\n",
    "    X_test = df_train.loc[infold_test_indices,df_train.columns != TARGET_COLUMN]\n",
    "    y_test = df_train.loc[infold_test_indices,TARGET_COLUMN]\n",
    "    \n",
    "    X_test = preprocessing(X_test)\n",
    "    X_test = postprocessing(X_test)\n",
    "\n",
    "\n",
    "    hyperparameters = {\n",
    "    'lambda_l1': 0.0019308425132961418,\n",
    "    'lambda_l2': 8.612376766701301,\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.5799027628764685,\n",
    "    'feature_fraction': 0.7594543478006752,\n",
    "    'learning_rate': 0.11960025124188359,\n",
    "    'verbosity': 8,\n",
    "    'max_depth': 6}\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        **hyperparameters,\n",
    "        callbacks=[EarlyStopping(rounds=50, min_delta=2e-4, maximize=False)],\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric=get_rmsle,\n",
    "    )\n",
    "\n",
    "    y_train_preds = model.predict(X_train)\n",
    "    train_rmse = root_mean_squared_log_error(y_true=y_train,y_pred=y_train_preds)\n",
    "    list_train_rmse.append(train_rmse)\n",
    "\n",
    "    y_test_preds = model.predict(X_test)\n",
    "    test_rmse = root_mean_squared_log_error(y_true=y_test,y_pred=y_test_preds)\n",
    "    list_test_rmse.append(test_rmse)\n",
    "\n",
    "    print(f'--- Fold {fold_number} Completed ---')\n",
    "    print('train_rmse, test_rmse - ',train_rmse,test_rmse)\n",
    "\n",
    "print('--- Training_Completed ---')\n",
    "print('The average test cross neg_root_mean_squared_error is ', sum(list_test_rmse)/len(list_test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66506282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average test cross neg_root_mean_squared_error is  0.06991284918127437\n"
     ]
    }
   ],
   "source": [
    "print('The average test cross neg_root_mean_squared_error is ', sum(list_test_rmse)/len(list_test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09df8790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=3 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=1.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=0.9 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=3 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=1.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=0.9 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.062148\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000001 seconds, init for row-wise cost 0.005462 seconds\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 750000, number of used features: 8\n",
      "[LightGBM] [Debug] Use subset for bagging\n",
      "[LightGBM] [Info] Start training from score 77.000000\n",
      "[LightGBM] [Debug] Re-bagging, using 434460 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 58 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 61 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 56 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 61 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 62 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 435327 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 61 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 59 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 59 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 434280 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 53 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 62 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 56 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 55 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 434899 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 61 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 56 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 58 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 55 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 55 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 434986 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 59 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 54 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 60 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 59 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 434971 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 60 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 60 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 61 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 58 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 49 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 434846 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 49 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 52 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 54 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 54 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 61 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 434606 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 54 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 56 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 49 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 56 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 50 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 435894 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 48 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 59 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 62 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 48 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 59 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 435429 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 36 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 61 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 35 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 61 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 434994 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 43 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 45 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 46 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 435072 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 46 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 47 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 49 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 54 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 56 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 434296 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 54 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 53 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 51 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 60 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 435195 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 61 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 63 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 58 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 61 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 434906 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 60 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 63 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 55 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 434946 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 63 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 59 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 63 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 435278 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 63 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 61 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 63 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 63 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 434623 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 63 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 61 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 55 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 58 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 434217 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 62 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 62 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 6\n",
      "[LightGBM] [Debug] Re-bagging, using 435530 data to train\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 60 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 58 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 63 and depth = 6\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 63 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(bagging_fraction=0.5799027628764685, bagging_freq=5,\n",
       "              colsample_bytree=0.5, feature_fraction=0.7594543478006752,\n",
       "              importance_type=&#x27;gain&#x27;, lambda_l1=0.0019308425132961418,\n",
       "              lambda_l2=8.612376766701301, learning_rate=0.11960025124188359,\n",
       "              max_depth=6, min_child_samples=10, num_leaves=64,\n",
       "              objective=&#x27;regression_l1&#x27;, random_state=64, reg_alpha=0.1,\n",
       "              reg_lambda=1.0, subsample=0.9, subsample_freq=3, verbosity=8)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LGBMRegressor</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMRegressor(bagging_fraction=0.5799027628764685, bagging_freq=5,\n",
       "              colsample_bytree=0.5, feature_fraction=0.7594543478006752,\n",
       "              importance_type=&#x27;gain&#x27;, lambda_l1=0.0019308425132961418,\n",
       "              lambda_l2=8.612376766701301, learning_rate=0.11960025124188359,\n",
       "              max_depth=6, min_child_samples=10, num_leaves=64,\n",
       "              objective=&#x27;regression_l1&#x27;, random_state=64, reg_alpha=0.1,\n",
       "              reg_lambda=1.0, subsample=0.9, subsample_freq=3, verbosity=8)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.5799027628764685, bagging_freq=5,\n",
       "              colsample_bytree=0.5, feature_fraction=0.7594543478006752,\n",
       "              importance_type='gain', lambda_l1=0.0019308425132961418,\n",
       "              lambda_l2=8.612376766701301, learning_rate=0.11960025124188359,\n",
       "              max_depth=6, min_child_samples=10, num_leaves=64,\n",
       "              objective='regression_l1', random_state=64, reg_alpha=0.1,\n",
       "              reg_lambda=1.0, subsample=0.9, subsample_freq=3, verbosity=8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training on entire dataset\n",
    "X_train = df_train.loc[:,df_train.columns != TARGET_COLUMN]\n",
    "y_train = df_train.loc[:,TARGET_COLUMN]\n",
    "\n",
    "X_train = preprocessing(X_train)    \n",
    "X_train = postprocessing(X_train)\n",
    "\n",
    "# Pre-processing of training data in kfold for in-fold validation\n",
    "X_test = df_test\n",
    "\n",
    "X_test = preprocessing(X_test)\n",
    "X_test = postprocessing(X_test)\n",
    "\n",
    "model = lgb.LGBMRegressor(\n",
    "    **hyperparameters,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    # eval_metric=get_rmsle,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9d712",
   "metadata": {},
   "source": [
    "# Test Set Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e230e19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=3 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=0.9 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=1.0 will be ignored. Current value: lambda_l2=8.612376766701301\n"
     ]
    }
   ],
   "source": [
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7ff3f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Calories</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>750000</th>\n",
       "      <td>88.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750001</th>\n",
       "      <td>88.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750002</th>\n",
       "      <td>88.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750003</th>\n",
       "      <td>88.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750004</th>\n",
       "      <td>88.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>88.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>88.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>88.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>88.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>88.283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Calories\n",
       "id              \n",
       "750000    88.283\n",
       "750001    88.283\n",
       "750002    88.283\n",
       "750003    88.283\n",
       "750004    88.283\n",
       "...          ...\n",
       "999995    88.283\n",
       "999996    88.283\n",
       "999997    88.283\n",
       "999998    88.283\n",
       "999999    88.283\n",
       "\n",
       "[250000 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5127c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "date = datetime.datetime.now().date().strftime('%Y-%m-%d')\n",
    "\n",
    "model_type = type(model).__name__\n",
    "\n",
    "comment = 'gender_column_fixed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "173bad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = df_sample_submission.copy()\n",
    "df_submission[TARGET_COLUMN] = y_preds.clip(min=0,max=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "642df921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.934170407006583)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission.Calories.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f299fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the csv to the submissions folder\n",
    "df_submission.to_csv(f'../submissions/{date}-{model_type}-{comment}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "438a2b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Calories</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>750000</th>\n",
       "      <td>27.509363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750001</th>\n",
       "      <td>108.949776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750002</th>\n",
       "      <td>86.906671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750003</th>\n",
       "      <td>129.452372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750004</th>\n",
       "      <td>76.875991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>26.366664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>9.071335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>73.115843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>166.814294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>74.155740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Calories\n",
       "id                \n",
       "750000   27.509363\n",
       "750001  108.949776\n",
       "750002   86.906671\n",
       "750003  129.452372\n",
       "750004   76.875991\n",
       "...            ...\n",
       "999995   26.366664\n",
       "999996    9.071335\n",
       "999997   73.115843\n",
       "999998  166.814294\n",
       "999999   74.155740\n",
       "\n",
       "[250000 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09e250df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lgb.plot_importance(model, importance_type=\"gain\", figsize=(7,6), title=\"LightGBM Feature Importance (Gain)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5524d49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cbutler/repos/personal_repos/KAGGLE_S5E5/.venv/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.0019308425132961418, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0.0019308425132961418\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=3 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7594543478006752, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.7594543478006752\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5799027628764685, subsample=0.9 will be ignored. Current value: bagging_fraction=0.5799027628764685\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.612376766701301, reg_lambda=1.0 will be ignored. Current value: lambda_l2=8.612376766701301\n",
      "🏃 View run secretive-stoat-964 at: http://localhost:5000/#/experiments/4/runs/4326035b0d314909a2cf3b9c6b4ba659\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/4\n"
     ]
    }
   ],
   "source": [
    "# CHECKLIST BEFORE RUNNING\n",
    "# 1. is this a new run (start_run run_id empty) or are you inserting into an old run (start run populated)\n",
    "# 2. Do you know the kaggle leaderboard metric? If not set to 999\n",
    "# 3. Is this a leaderboard model? If not then disable the model logging at the end\n",
    "# This take 2 minutes to run\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"Kaggle S5E5\")\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(hyperparameters)\n",
    "\n",
    "    # Log the loss metric\n",
    "    mlflow.log_metric(\"cv_score\", sum(list_test_rmse)/len(list_test_rmse))\n",
    "    mlflow.log_metric(\"kaggle leaderboard\", 0.16078)\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(\n",
    "        model_input=X_train,\n",
    "        model_output=y_train,\n",
    "    )\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=\"model\",\n",
    "        signature=signature,\n",
    "        input_example=X_train,\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24bf6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
